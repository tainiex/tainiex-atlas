import { Injectable, Inject } from '@nestjs/common';
import type {
  IVectorStore,
  IVectorStoreRecord,
} from '../../../shared-atlas/src/interfaces/vector-store.interface';
import { LlmService } from '../../llm/llm.service';
import { MemoryType, MemorySource } from './entities/memory.entity';
import { WorkerPoolService } from '../worker/worker-pool.service';
import {
  DistillationConfig,
  DistillationResult,
  DistillationPayload,
} from '../worker/worker.interfaces';
import { ConfigService } from '@nestjs/config';
import { GraphService } from '../../graph/graph.service';
import { LoggerService } from '../../common/logger/logger.service';

@Injectable()
export class MemoryService {
  private queue: { verify: () => void; task: () => Promise<void> }[] = [];
  private activeWorkers = 0;
  private readonly maxWorkers: number;

  constructor(
    @Inject('IVectorStore')
    private vectorStore: IVectorStore,
    private llmService: LlmService,
    private configService: ConfigService,
    private graphService: GraphService,
    private workerPool: WorkerPoolService,
    private logger: LoggerService,
  ) {
    this.logger.setContext(MemoryService.name);
    this.maxWorkers = parseInt(
      this.configService.get('DISTILLATION_CONCURRENCY') || '4',
      10,
    );
  }

  async addMemory(
    userId: string,
    sourceType: MemorySource,
    sourceId: string,
    content: string,
    type: MemoryType = MemoryType.PERSONAL,
    entities: Record<string, any> = {},
    importance: number = 1,
  ) {
    try {
      const embedding = await this.llmService.getEmbeddings(content);

      const record: IVectorStoreRecord = {
        // eslint-disable-next-line @typescript-eslint/no-unsafe-assignment
        id: undefined as any, // ID generated by DB
        content,
        embedding,
        metadata: {
          userId,
          sourceType,
          sourceId,
          type,
          entities,
          importance,
          tags: [],
        },
      };

      await this.vectorStore.add(userId, [record]);
      this.logger.log(`[MemoryService] Added memory for user ${userId}`);
    } catch (error) {
      this.logger.error(
        'Failed to add memory',
        error instanceof Error ? error.stack : String(error),
      );
      throw error;
    }
  }

  async searchMemories(
    userId: string,
    query: string,
    limit: number = 5,
    similarityThreshold: number = 0.6, // Default threshold | 默认阈值
  ): Promise<IVectorStoreRecord[]> {
    try {
      const queryEmbedding = await this.llmService.getEmbeddings(query);

      /**
       * Strategy: Broad Retrieval + Re-ranking
       * 策略：扩大召回 + 重排序
       *
       * 1. Retrieve more candidates (3x limit) from VectorDB based on raw distance.
       * 1. 从向量数据库基于原始距离召回更多候选集（3倍 limit）。
       *
       * 2. Re-rank in memory using:
       * 2. 在内存中基于以下维度重排序：
       *    - Vector Similarity (余弦相似度): 0.7 weight
       *    - Importance (重要性): 0.2 weight (normalized 1-3 -> 0.33-1.0)
       *    - Recency (时效性): 0.1 weight (decay function)
       */
      const candidateLimit = limit * 3;
      // Get candidates using raw vector search
      // 使用原始向量搜索获取候选集
      const candidates = await this.vectorStore.search(
        userId,
        queryEmbedding,
        candidateLimit,
      );

      if (candidates.length === 0) return [];

      // Helper: Calculate Cosine Similarity
      // 辅助函数：计算余弦相似度
      // Formula 公式: (A . B) / (||A|| * ||B||)
      const calculateCosineSimilarity = (
        vecA: number[],
        vecB: number[],
      ): number => {
        let dotProduct = 0;
        let normA = 0;
        let normB = 0;
        for (let i = 0; i < vecA.length; i++) {
          dotProduct += vecA[i] * vecB[i];
          normA += vecA[i] * vecA[i];
          normB += vecB[i] * vecB[i];
        }
        if (normA === 0 || normB === 0) return 0;
        return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
      };

      const now = new Date();
      const oneDayMs = 24 * 60 * 60 * 1000;

      const scoredCandidates = candidates.map((memory) => {
        // 1. Similarity Score (0.0 - 1.0)
        // 1. 相似度得分
        const similarity = calculateCosineSimilarity(
          queryEmbedding,
          memory.embedding,
        );

        // 2. Importance Score (Normalized to 0.0 - 1.0)
        // 2. 重要性得分（归一化）
        // Map importance 1-3 to 0.33, 0.66, 1.0
        // 将重要性 1-3 映射为 0.33, 0.66, 1.0
        // eslint-disable-next-line @typescript-eslint/no-unsafe-assignment
        const rawImportance: number = memory.metadata.importance || 1;
        // Cap at 3 for normalization base
        // 上限设为 3 进行归一化
        const importanceScore = Math.min(rawImportance, 3) / 3;

        // 3. Recency Score (Decay Function)
        // 3. 时效性得分（衰减函数）
        // Formula: 1 / (1 + decayRate * daysPassed)
        // eslint-disable-next-line @typescript-eslint/no-unsafe-argument
        const createdAt = new Date(memory.metadata.createdAt || now);
        const daysPassed = Math.max(
          0,
          (now.getTime() - createdAt.getTime()) / oneDayMs,
        );
        const decayRate = 0.1; // Adjustable decay rate | 可调节的衰减率
        const recencyScore = 1 / (1 + decayRate * daysPassed);

        // 4. Final Weighted Score
        // 4. 最终加权得分
        // Sim: 0.7, Imp: 0.2, Rec: 0.1
        const finalScore =
          similarity * 0.7 + importanceScore * 0.2 + recencyScore * 0.1;

        return {
          ...memory,
          _score: finalScore,
          _similarity: similarity,
          _debug: {
            daysPassed: daysPassed.toFixed(1),
            recency: recencyScore.toFixed(2),
            importance: importanceScore.toFixed(2),
            sim: similarity.toFixed(3),
          },
        };
      });

      // Filter and Sort
      // 过滤与排序
      const results = scoredCandidates
        .filter((m) => m._similarity >= similarityThreshold) // Filter by similarity threshold | 相似度阈值过滤(absolute min)
        .sort((a, b) => b._score - a._score) // Sort desc by final score | 按最终得分降序
        .slice(0, limit); // Take top N | 取前 N 个

      this.logger.debug(
        `[MemorySearch] Query: "${query}" -> Top ${results.length} results (from ${candidates.length} candidates)`,
      );
      if (results.length > 0) {
        this.logger.debug(
          `[MemorySearch] Top Match: Score=${results[0]._score.toFixed(3)} (Sim=${results[0]._similarity.toFixed(3)}, Imp=${results[0].metadata.importance}, Age=${results[0]._debug.daysPassed}d)`,
        );
      }

      return results;
    } catch (error) {
      this.logger.error('Failed to search memories', error);
      return [];
    }
  }

  distillConversation(
    userId: string,
    sessionId: string,
    messages: { role: string; content: string }[],
  ) {
    if (messages.length === 0) return;

    // Push to queue
    this.queue.push({
      verify: () => {}, // Placeholder if we need cancellation later
      task: async () => this.runDistillationTask(userId, sessionId, messages),
    });

    void this.processQueue();
  }

  private processQueue() {
    if (this.activeWorkers >= this.maxWorkers) {
      return;
    }

    const item = this.queue.shift();
    if (!item) return;

    this.activeWorkers++;

    // Don't await the task here, let it run in background

    void item.task().finally(() => {
      this.activeWorkers--;

      void this.processQueue(); // Process next item
    });
  }

  private async runDistillationTask(
    userId: string,
    sessionId: string,
    messages: { role: string; content: string }[],
  ) {
    try {
      const conversationText = messages
        .map((m) => `${m.role}: ${m.content}`)
        .join('\n');
      const prompt = `
Task: Analyze the provided conversation context and distill high-value, long-term memories.
CRITICAL: Do NOT just summarize the conversation events. Instead, extract underlying truths, insights, patterns, and strong opinions.

**NEW: Knowledge Graph Extraction (知识图谱提取)**
You must now also extract "Entities" (Key Concepts) and "Relations" (Connections) for each memory to build a semantic graph.
- **Entities**: Technical keywords, projects, libraries, or abstract concepts mentioned (e.g., "Next.js", "SSR", "Cost").
- **Relations**: How these entities relate to the user or each other (e.g., "User -> prefers -> Next.js").

**CRITICAL ROLE AWARENESS:**
The conversation contains TWO participants:
- "user": The actual user (YOUR PRIMARY FOCUS)
- "assistant": The AI assistant (IGNORE ITS SUGGESTIONS)

RULES:
✅ ONLY extract insights from what the user explicitly said or decided
❌ DO NOT attribute assistant's recommendations to the user
❌ DO NOT create memories like "User thinks X" if only assistant suggested X

Role Confusion Prevention:
If conversation shows:
  assistant: "I recommend using Vercel"
  user: "Okay" (without explicitly agreeing)
→ DO NOT create: "User prefers Vercel"
→ This is assistant's opinion, not user's preference

**CRITICAL DECISION AWARENESS:**
Distinguish between EXPLORATION and CONCLUSION:

EXPLORATION (探索阶段) - Lower priority, avoid if possible:
- "I'm considering A or B" / "我在考虑A或B"
- "What about X?" / "X怎么样？"
- "A has pros, B has cons" / "A有优点，B有缺点"
→ User is still thinking, NO decision made yet

CONCLUSION (决策阶段) - Higher priority, MUST capture:
- "I decided to use X" / "我决定用X"
- "I'll go with Y because..." / "我选择Y，因为..."
- "After considering, I choose Z" / "考虑后，我选Z"
→ User made a final decision, CAPTURE THIS

Signal Words for Decisions (决策信号词):
- Chinese: 决定、选择、确定、最终、采用、用、就用
- English: decided, chosen, will use, going with, settled on, pick

Extraction Priority:
1. HIGHEST: Final decisions with reasoning
2. MEDIUM: Strong preferences ("I prefer X over Y because...")
3. LOWEST: Options being explored ("considering A or B")
4. AVOID: Questions without conclusions

Temporal Priority Rule:
If conversation shows evolution from exploration to decision:
Early: "I'm thinking about A, B, C"
Later: "I decided on B"
→ ONLY extract the final decision (B)
→ IGNORE early exploration phase unless it reveals preference patterns

**OUTPUT CONSTRAINTS:**
CRITICAL: Extract ONLY 1-3 memories per distillation cycle
- Maximum 3 memories — if you have more, you are doing it WRONG
- Minimum importance threshold: 2 (DO NOT create memories with importance < 2)
- Quality over Quantity: Better to extract 1 perfect memory than 5 mediocre ones
- If conversation is trivial/exploratory with no strong conclusions, return EMPTY array []

Importance Level Guidelines:
IMPORTANCE 3 (Critical - ALWAYS extract if present):
- Final decisions on major technical choices
- Strong personal preferences with reasoning
- Proven solutions to recurring problems
- Architectural decisions with long-term impact

IMPORTANCE 2 (Medium - Extract if truly valuable):
- Secondary preferences
- Learned patterns
- Non-critical but meaningful decisions

IMPORTANCE 1 (Low - NEVER extract):
- Transient concerns
- Exploratory questions without conclusions
- Factual information without user stance

Rule: If in doubt, DO NOT extract. Err on the side of fewer, higher-quality memories.

Focus on extracting:
1. **User Philosophies & Preferences**: The user's deep-seated beliefs about coding, design, or workflow (e.g., "Prefers composition over inheritance", "Strongly dislikes verbose logging", "Values aesthetics over performance"). -> Type: PERSONAL
2. **Technical Insights & Learnings**: Proven solutions or conclusions reached about specific technical problems, including the "Why" (e.g., "WebSockets require heartbeat on mobile due to aggressive OS background suspension"). -> Type: DOMAIN
3. **Project Decisions & Context**: Critical architectural decisions and their rationale. -> Type: TASK

Avoid:
- Trivial facts ("User sent a message about X").
- Transient debugging state ("User is fixing a bug in loop").
- Repetition of logs or error messages.

**Language Rule**: The \`content\` field MUST be in the same language as the User's messages in the verified conversation. If the user speaks Chinese, the memory content MUST be in Chinese. If English, use English.

**Anti-Patterns (禁止的输出形式):**
❌ WRONG: "Firebase提供10GB流量，Vercel提供100GB流量"
   → 纯事实陈述，缺少用户立场

❌ WRONG: "用户询问了关于托管方案的问题"
   → 流程描述，无价值

❌ WRONG: "对话讨论了Firebase和Vercel两种方案的优劣"
   → 摘要式总结

**Correct Patterns (正确的输出形式):**
✅ RIGHT: "用户倾向选择流量充裕的托管方案（重视Vercel 100GB over Firebase 10GB，看重爆款承载能力）"
   → 明确的偏好和决策理由

✅ RIGHT: "用户认为部署速度和图片优化是选择托管方案的关键因素"
   → 提炼出的价值判断

**Golden Rule:** Ask yourself "What did the user DECIDE, PREFER, or LEARN?" not "What did they SAY?"
**黄金准则：** 问自己"用户**决定、偏好或学到了什么**"，而不是"用户**说了什么**"

Output Format: JSON Array of objects.
Allowed Types: "PERSONAL", "TASK", "DOMAIN"

Example with Decision Awareness:

Conversation:
user: "我在纠结用Firebase还是Vercel"
assistant: "Firebase便宜，Vercel功能强"
user: "嗯，流量限制是个问题"
assistant: "是的，Vercel有100GB"
user: "好的，那我就用Vercel了，虽然贵点但流量够"

❌ WRONG Output (Too many, low importance):
[
  { "content": "用户询问了Firebase和Vercel的对比", "importance": 1 },
  { "content": "用户了解了两者的价格差异", "importance": 1 },
  { "content": "用户关心流量限制问题", "importance": 2 },
  { "content": "Firebase提供的流量较少", "importance": 1 },
  { "content": "Vercel提供100GB流量", "importance": 1 },
  { "content": "用户决定使用Vercel", "importance": 2 },
  { "content": "用户接受了更高的成本", "importance": 1 },
  { "content": "用户重视流量扩展性", "importance": 2 }
]
→ Problem: 8 items! Violates max=3 rule, many importance=1 (should be excluded)

✅ CORRECT Output (Focused, high importance only):
[
  { 
    "content": "用户选择Vercel作为托管方案（权衡后认为流量限制比成本更重要）", 
    "type": "TASK", 
    "importance": 3,
    "entities": ["Vercel", "Firebase", "Hosting", "Scalability"],
    "relations": ["User -> chose -> Vercel", "User -> values -> Scalability"]
  },
  { 
    "content": "用户在技术选型时优先考虑可扩展性（流量承载能力）over 成本", 
    "type": "PERSONAL", 
    "importance": 3,
    "entities": ["Scalability", "Cost", "Trade-off"],
    "relations": ["User -> prioritizes -> Scalability"]
  }
]
→ Success: 2 items, both importance=3, captures the essence + Graph Data

Additional Wrong Outputs to AVOID:
❌ "用户在考虑Firebase和Vercel" (exploration phase, not decision)
❌ "Vercel提供100GB流量" (factual statement from assistant)
❌ "AI建议使用Vercel" (assistant's action, not user's)

Conversation:
${conversationText}

Output JSON (only JSON, no markdown):
`;

      const targetModel =
        this.configService.get<string>('MEMORY_DISTILLATION_MODEL') ||
        'gemini-2.0-flash-001';

      // 2. Prepare Config for Worker

      const workerConfig: DistillationConfig = {
        VERTEX_PROJECT_ID:
          this.configService.get<string>('VERTEX_PROJECT_ID') || '',
        VERTEX_LOCATION:
          this.configService.get<string>('VERTEX_LOCATION') || 'us-central1',
        GSA_KEY_FILE: this.configService.get<string>('GSA_KEY_FILE'),
        DB_HOST: this.configService.get<string>('DB_HOST'),
        DB_PORT: this.configService.get<string>('DB_PORT'),
        DB_USERNAME: this.configService.get<string>('DB_USERNAME'),
        DB_PASSWORD: this.configService.get<string>('DB_PASSWORD'),
        DB_NAME: this.configService.get<string>('DB_NAME'),
        DB_SSL: this.configService.get<string>('DB_SSL'),
      };

      // 3. Submit to Worker Pool
      const output = await this.workerPool.run<
        DistillationPayload,
        DistillationResult
      >('distillConversation', {
        config: workerConfig,
        prompt: prompt,
        modelName: targetModel,
      });

      if (output.error) {
        this.logger.error('[MemoryService] Worker Error:', output.error);
        return;
      }

      const memories = (output.result || []) as {
        content: string;
        type?: string;
        importance?: number;
        entities?: string[];
        relations?: string[];
      }[];
      if (memories.length === 0) return;

      // 4. Process Results
      for (const m of memories) {
        if (m.content) {
          // Normalize type
          let memoryType = MemoryType.PERSONAL;
          if (m.type === 'TASK') memoryType = MemoryType.TASK;
          if (m.type === 'DOMAIN') memoryType = MemoryType.DOMAIN;

          // Extract Graph Data
          const graphMetadata = {
            entities: Array.isArray(m.entities) ? m.entities : [],
            relations: Array.isArray(m.relations) ? m.relations : [],
          };

          await this.addMemory(
            userId,
            MemorySource.CHAT,
            sessionId,
            m.content,
            memoryType,
            graphMetadata,
            m.importance || 1,
          );
        }
      }
    } catch (error) {
      this.logger.error(
        `[MemoryService] Distillation failed for session ${sessionId}:`,
        error,
      );
    }
  }

  /**
   * Recursive Backfill Logic
   * Returns the ID of the last processed message, or null if no messages were processed (caught up).
   */
  async processBackfillChunk(
    userId: string,
    sessionId: string,
    lastProcessedMsgId: string | null,
    messageFetcher: (
      sessionId: string,
      lastMsgId: string | null,
      limit: number,
    ) => Promise<{ id: string; role: string; content: string }[]>,
  ): Promise<string | null> {
    try {
      // 1. Fetch next chunk
      const limit = parseInt(
        this.configService.get('DISTILL_BACKFILL_CHUNK_SIZE') || '30',
        10,
      );
      const messages = await messageFetcher(
        sessionId,
        lastProcessedMsgId,
        limit,
      );

      if (messages.length === 0) {
        this.logger.debug(
          `[MemoryService] [Backfill] No more messages to process for ${sessionId}. Backfill Complete.`,
        );
        return null; // Done
      }

      // 2. Distill
      const startId = messages[0].id;
      const endId = messages[messages.length - 1].id;
      this.logger.log(
        `[MemoryService] [Backfill] Processing chunk for ${sessionId}. Size: ${messages.length}. Range: ${startId} ... ${endId}`,
      );

      await this.runDistillationTask(userId, sessionId, messages); // Re-use existing logic

      // 3. Return checkpoint (last message ID)
      const newCheckpoint = endId;
      this.logger.debug(
        `[MemoryService] [Backfill] Chunk finished. New Checkpoint: ${newCheckpoint}`,
      );
      return newCheckpoint;
    } catch (error) {
      this.logger.error(
        `[MemoryService] Backfill chunk failed for session ${sessionId}`,
        error,
      );
      throw error; // Propagate to retry or fail the job
    }
  }
}
